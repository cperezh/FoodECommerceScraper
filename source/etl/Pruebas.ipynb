{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e7d27b-384c-4596-85a4-91fc5e2b981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./source/etl'))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from SparkDBUtils import SparkDB\n",
    "import delta\n",
    "import datetime as dt\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import DateType, StructType, StructField, IntegerType, TimestampType\n",
    "\n",
    "sparkdb = SparkDB()\n",
    "spark = sparkdb.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b456abb3-9710-4723-9ba2-1abc72341397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|id_date|      date|             ts_load|\n",
      "+-------+----------+--------------------+\n",
      "|      1|2022-11-21|2023-02-23 09:52:...|\n",
      "|      2|2022-11-23|2023-02-23 09:52:...|\n",
      "|      3|2022-11-24|2023-02-23 09:52:...|\n",
      "|      4|2022-11-25|2023-02-23 09:52:...|\n",
      "|      5|2022-11-26|2023-02-23 09:52:...|\n",
      "|      6|2022-11-27|2023-02-23 09:52:...|\n",
      "|      7|2022-11-28|2023-02-23 09:52:...|\n",
      "|      8|2022-11-29|2023-02-23 09:52:...|\n",
      "|      9|2022-12-01|2023-02-23 09:52:...|\n",
      "|     10|2022-12-03|2023-02-23 09:52:...|\n",
      "|     11|2022-12-06|2023-02-23 09:52:...|\n",
      "|     12|2022-12-08|2023-02-23 09:52:...|\n",
      "|     13|2022-12-10|2023-02-23 09:52:...|\n",
      "|     14|2022-12-11|2023-02-23 09:52:...|\n",
      "|     15|2022-12-12|2023-02-23 09:52:...|\n",
      "|     16|2022-12-13|2023-02-23 09:52:...|\n",
      "|     17|2022-12-19|2023-02-23 09:52:...|\n",
      "|     18|2022-12-22|2023-02-23 09:52:...|\n",
      "|     19|2022-12-23|2023-02-23 09:52:...|\n",
      "|     20|2022-12-24|2023-02-23 09:52:...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from date_dim\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4bd3f2-d9da-4368-b484-635bfc36da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+--------------------+\n",
      "|table_name| id|             ts_load|\n",
      "+----------+---+--------------------+\n",
      "|  date_dim| 39|2023-02-23 09:50:...|\n",
      "+----------+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from sequences_cfg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6174a15b-d00c-43f3-b6af-580a1e0516fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|num_affected_rows|\n",
      "+-----------------+\n",
      "|               39|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"delete from date_dim\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32e603e8-b88f-466b-b2d5-7261a7bdf105",
   "metadata": {},
   "outputs": [],
   "source": [
    " def insert_id(df: pyspark.sql.dataframe, table_name: str) -> pyspark.sql.dataframe:\n",
    "\n",
    "        # Ventana por cualquier columna, para poder usar row_number\n",
    "        window_spec = pyspark.sql.window.Window \\\n",
    "            .orderBy(df_new.columns[0])\n",
    "\n",
    "        # Obtenemos la ultima secuencia que se utiliz√≥\n",
    "        seq = sparkdb.read_last_seq(table_name)\n",
    "\n",
    "        # Actualizamos la columna id con secuenciales desde la ultima secuencia\n",
    "        df = df. \\\n",
    "            withColumn(\"id\", f.row_number().over(window_spec) + seq)\n",
    "\n",
    "        # Obtenemos la nueva ultima secuencia\n",
    "        max_seq = df.pandas_api()[\"id\"].max()\n",
    "        \n",
    "        # Actualizamos en la tabla de secuencias\n",
    "        spark.sql(f\"\"\"\n",
    "            update sequences_cfg set id={max_seq} \n",
    "            where table_name == '{table_name}'\n",
    "            \"\"\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b4cfd45-42ce-4d56-9508-17206f355b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\\\n",
    "        StructField(\"id\", IntegerType(), True),\\\n",
    "        StructField(\"date\", DateType(), True),\\\n",
    "        StructField(\"ts_load\", TimestampType(), True),\\\n",
    "        ])\n",
    "\n",
    "df_new = sparkdb.spark.createDataFrame([\n",
    "        (None, dt.datetime(2020, 5, 17), dt.datetime.now()),\n",
    "        (None, dt.datetime(2020, 5, 25), dt.datetime.now())],\n",
    "        schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "448ecd48-32c7-45b2-ac24-b0b07d2db8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "df_new = insert_id(df_new,\"date_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "395797e2-0523-4421-8f0a-ffda230c6f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|      date|             ts_load|\n",
      "+---+----------+--------------------+\n",
      "| 11|2020-05-17|2023-02-22 07:16:...|\n",
      "| 12|2020-05-25|2023-02-22 07:16:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60eea965-79be-4bf9-8cb9-94ced5fa7066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|      date|             ts_load|\n",
      "+---+----------+--------------------+\n",
      "|  7|2020-05-17|2023-02-22 07:16:...|\n",
      "|  8|2020-05-25|2023-02-22 07:16:...|\n",
      "| 11|2020-05-17|2023-02-22 07:16:...|\n",
      "| 12|2020-05-25|2023-02-22 07:16:...|\n",
      "|  5|2020-05-17|2023-02-22 07:16:...|\n",
      "|  6|2020-05-25|2023-02-22 07:16:...|\n",
      "|  9|2020-05-17|2023-02-22 07:16:...|\n",
      "| 10|2020-05-25|2023-02-22 07:16:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkdb.write_table(df_new, \"date_dim\", \"append\")\n",
    "\n",
    "sparkdb.read_table(\"date_dim\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812bbdcf-e4ac-4a24-977c-88842c3b00bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
